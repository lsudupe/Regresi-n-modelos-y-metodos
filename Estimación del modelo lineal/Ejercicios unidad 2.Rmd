---
title: "Estimación del modelo lineal ejercicios"
author: "Laura Sudupe Medinilla"
date: "1/3/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, echo = FALSE, message = FALSE}
# Función para cargar todos los paquetes necesarios
LoadLibraries <- function() {
  myLibraries <- c("faraway", "ggplot2", "reshape2")
  invisible(lapply(myLibraries, library, character.only = TRUE))
}
LoadLibraries()
```


# Ejercicios faraway

## 1.
  The dataset teengamb concerns a study of teenage gambling in Britain. Fit a 
  regression model with the expenditure on gambling as the response and the sex, 
  status, income and verbal score as predictors. Present the output.
```{r}
#ajustamos los datos al modelo
lm <- lm(gamble ~ sex + status + income + verbal, data=teengamb)
lmsum <- summary(lm)
lmsum
```
  (a) What percentage of variation in the response is explained by these 
  predictors?
```{r}
#El coeficiente de determinación R^2 es igual al cuadrado de la correlación del 
#coeficiente. Cuando se expresa en porcentaje, R^2 representa el porcentaje de
#variación en la variable dependiente y puede ser explicado mediante la 
#variación en la variable independiente x mediante la linea de regreión.

(lmsum$r.squared)*100
```
    (b) Which observation has the largest (positive) residual? Give the case 
  number.
```{r}
residuos <- lmsum$residuals
max(residuos)      #valor residuo maximo
which.max(residuos) #observacion con el maximo valor
```
  (c) Compute the mean and median of the residuals.
```{r}
#Han de dar valores entorno al cero, el hecho de que no den es debido a errores
#de calculo
mean(residuos)
median(residuos)
```
  (d) Compute the correlation of the residuals with the fitted values.
```{r}
cor(fitted(lm), resid(lm))
```
  (e) Compute the correlation of the residuals with the income.
```{r}
cor(resid(lm), teengamb$income)
```
  
  (f) For all other predictors held constant, what would be the difference in 
  predicted expenditure on gambling for a male compared to a female?
```{r}
lmsum
lm$coefficients["sex"]
```
  Estos datos nos indican la pendiente de la recta, por lo tanto, la diferencia
  entre "expenditure on gambling in punds per year" es 22.12 "pounds" distinta
  para los hombres que para las mujeres. Menor para las mujeres (sex=1)
  

## 2.
  The dataset uswages is drawn as a sample from the Current Population Survey in
  1988. 
  a) Fit a model with weekly wages as the response and years of education and
  experience as predictors. 
  
```{r}
lm2 <- lm(wage ~ educ + exper, data = uswages)
summary(lm2)
```
  b) Report and give a simple interpretation to the regression coefficient for 
  years of education. Now fit the same model but with logged weekly wages. 
```{r}
lm2$coefficients
```
  El ajuste no es muy bueno, tenemos un $R^2$ bajo y un error estandar elevado.
  
  Los coeficientes nos indican que el sueldo (wage) depende de la educación 
  recibida y la experiencia. La relación es de 51.18 unidades mas de educación 
  por cada wage y 9.77 unidades mas de experiencia por cada wage. Esto quiere
  decir que hay un aumento de 51.18 unidades en el salario por cada año de 
  educación y un aumento de 9.77 unidades en el salario por cada año de 
  experiencia.
  
  $$wage = -242.8 + 51.17educ + 9.77expe$$
  
```{r}
#modificamos los datos de waeg logaritmicamente
lmlog2 <- lm(log(wage) ~ educ + exper, data = uswages)
summary(lmlog2)
lmlog2$coefficients
```
 Vemos que $R^2$ ha aumentado ligeramente y el error estandar a bajado 
 considerablemente.
 
 La relación entre los coeficientes es la siguiente
 
 $$\log(wage) = 4.65 + 0.09educ + 0.02expe$$
  c)Give an interpretation to the regression coefficient for years of education.
  Which interpretation is more natural?
  No entiendo lo que se me pide
  
  
## 4.
  The dataset prostate comes from a study on 97 men with prostate cancer who 
  were due to receive a radical prostatectomy. Fit a model with lpsa as the 
  response and lcavol as the predictor. Record the residual standard error and 
  the R2
```{r}
lm4 <- lm(lpsa ~ lcavol, data=prostate)
summary(lm4)
```
```{r}
#Error estandar de los residups
res4.1 <- summary(lm4)$sigma

#R^2, coeficiente de determinación
coef4.1 <- summary(lm4)$r.squared
```
  . Now add lweight, svi, lbph, age, lcp, pgg45 and gleason to the model one at 
  a time. For each model record the residual standard error and the R2
```{r}
lm4.2 <- update(lm4,. ~. + lweight)
res4.2 <- summary(lm4.2)$sigma
coef4.2 <- summary(lm4.2)$r.squared

lm4.3 <- update(lm4.2,. ~. + svi)
res4.3 <- summary(lm4.3)$sigma
coef4.3 <- summary(lm4.3)$r.squared

lm4.4 <- update(lm4.3,. ~. + lbph)
res4.4 <- summary(lm4.4)$sigma
coef4.4 <- summary(lm4.4)$r.squared

lm4.5 <- update(lm4.4,. ~. + age)
res4.5 <- summary(lm4.5)$sigma
coef4.5 <- summary(lm4.5)$r.squared

lm4.6 <- update(lm4.5,. ~. + lcp)
res4.6 <- summary(lm4.6)$sigma
coef4.6 <- summary(lm4.6)$r.squared

lm4.7 <- update(lm4.6,. ~. + pgg45)
res4.7 <- summary(lm4.7)$sigma
coef4.7 <- summary(lm4.7)$r.squared

lm4.8 <- update(lm4.7,. ~. + gleason)
res4.8 <- summary(lm4.8)$sigma
coef4.8 <- summary(lm4.8)$r.squared


reserror <- c(res4.1, res4.2, res4.3, res4.4, res4.5, res4.6, res4.7, res4.8)
coef <- c(coef4.1, coef4.2, coef4.3, coef4.4, coef4.5, coef4.6, coef4.7, coef4.8)
variables <- (1:8)


```
  
  . Plot the trends in these two statistics.
```{r}
data <- data.frame(variables, reserror, coef)

data <- melt(data, id="variables")

ggplot(data=data, aes(x=variables, y=value, colour=variable)) +
  geom_line()
        
```
  
## 5.
  Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa
  on lcavol and lcavol on lpsa. Display both regression lines on the plot. 
  At what point do the two lines intersect?
```{r}
lm5.1 <- lm(lpsa ~ lcavol, data=prostate)
lm5.2 <- lm(lcavol ~ lpsa, data=prostate)

plot(lpsa ~ lcavol, data=prostate)
abline(lm5.1, col="red")
abline(lm5.2, col="blue")
```
  Las rectan son paralelas, pero cada recta tiene unos ejes distintos, aqui 
  estamos representando las dos rectas respecto a los ejes lpsa against lcavol.
  
  
## 6.
  Thirty samples of cheddar cheese were analyzed for their content of acetic 
  acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a
  panel of judges and the average taste score produced. Use the cheddar data to 
  answer the following:
  
  (a) Fit a regression model with taste as the response and the three chemical 
  contents as predictors. Report the values of the regression coefficients.
```{r}
lm6 <- lm(taste ~ Acetic + H2S + Lactic, data=cheddar)
coefcheddar <- lm6$coefficients
coefcheddar
```
  
  (b) Compute the correlation between the fitted values and the response. Square
  it. Identify where this value appears in the regression output.
```{r}
corr <- cor(cheddar$taste, lm6$fitted.values)^2
corr
summary(lm6)
```
  Es el valor de `Multiple R-squared`
  
  (c) Fit the same regression model but without an intercept term. What is the 
  value of R2 reported in the output? Compute a more reasonable measure of the 
  good-ness of fit for this example.
```{r}
lm6.2 <- lm(taste ~ 0 + Acetic + H2S + Lactic, data=cheddar)
summary(lm6.2)
```
  El valor de R-squared es `multiple R-squared = 0.8877`
  
  (d) Compute the regression coefficients from the original fit using the QR 
  decomposition showing your R code.
  
## 7.
  An experiment was conducted to determine the effect of four factors on the 
  resistivity of a semi-conductor wafer. The data is found in wafer where each 
  of the four factors is coded as − or + depending on whether the low or the 
  high setting for that factor was used. Fit the linear model 
  `resist ~ x1 + x2 + x3 + x4`.
```{r}
lm7 <- lm(resist ~ x1 + x2 + x3 + x4, data=wafer)
head(wafer, package="faraway")
```
  
  (a) Extract the X matrix using the model.matrix function. Examine this to 
  determine how the low and high levels have been coded in the model.
```{r}
matrix7 <- model.matrix(lm7)
matrix7
```
  Vemos que los valores `-` se han asignado a `0`y los `+`a `1`.
  
  (b) Compute the correlation in the X matrix. Why are there some missing values
  in the matrix?
```{r}
cor(matrix7)
cor
```
  Esto se puede deber a que los valores del ìntercept`son constantes para todos
```{r}
cor2 <- cor(matrix7[,2:5])
cor2
```
  
  (c) What difference in resistance is expected when moving from the low to the
  high level of x1?
```{r}
summary(lm7)
```
  Una diferencia de `25.76`unidades
  
  (d) Refit the model without x4 and examine the regression coefficients and 
  standard errors? What stayed the the same as the original fit and what changed?
```{r}
lm7.2 <- lm(resist ~ x1 + x2 + x3, data=wafer)
summary(lm7.2)
```
  Los coeficientes de las variables `x1,x2 y x3` se mantienen iguales. Tenemos
  un `Multiple R-squared`algo menor. El `intercept` estimado tambien cambia 
  ligeramente, pues ya no tiene en cuenta la dependencia de la variable `x4`
  
  (e) Explain how the change in the regression coefficients is related to the 
  correlation matrix of X.
  
  
# Ejercicios Carmona

##1. 
  Una variable Y toma los valores y1, y2 y y3 en función de otra variable X con
  los valores x1, x2 y x3. Determinar cuales de los siguientes modelos son 
  lineales y encontrar, en su caso, la matriz de diseño para x1 = 1, x2 = 2 y 
  x3 = 3.
  
  a) $$y_{i} = \beta_{0} + \beta_{1}x_{i} + \beta_{2}(x_{i}^2 - 1) + \epsilon_{i}$$
  
  Es lineal
```{r}
matriza <- matrix(c(1,1,(1^2 -1),1,2,(2^2 -1),1,3,(3^2 -1)), nrow=3, byrow=TRUE)
matriza
```
  
  b) $$y_{i} = \beta_{0} + \beta_{1}x_{i} + \beta_{2}e^{xi} + \epsilon_{i}$$
  
  Es lineal
```{r}
matrizb <- matrix(c(1,1,exp(1),1,2,exp(2),1,3,exp(3)), nrow=3, byrow=TRUE)
matrizb
```
 
 c) $$y_{i} = \beta_{1}x_{i}(\beta_{2}tan(xi)) + \epsilon_{i}$$
  
  No es lineal
  
  
## 2.
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  